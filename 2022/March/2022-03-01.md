# March 1, 2022

I spent yesterday looking at how things get marshalled to put into MongoDB.
Along the way I realized the container doesn't seem to be working quite the way
I want (storage on the host with Mongo running _inside_ the container.) I need
to fix that, but for now it honestly doesn't matter since I don't need/want to
preserve what I'm doing - yet.

I am able to talk to MongoDB and I can create documents.  What I now need to
figure out is how to convert the C# data structures into BSON.  While this can
be a somewhat automated process, I have a specific model for how I want the data
structured and my goal is to make it easy to extend the model.

The model right now is one where I consider everything to be an "activity
record".  Such an activity record should have the following explicit fields:

* A "SourceIdentifer," which is a UUID that represents the specific activity
  record.
* A "SourceVersion," which represents the version of the agent that created the
  activity record.
* A "Timestamp," which represents the UTC time of the local system when the
  record was created.
* A list of attribute/value pairs, which represent extracted information from
  the record.
* A record of the raw data; this should correspond with the SourceIdentifier and
  SourceVersion so that we can capture data, even if we do not extract
  attributes from it at the ime the data is collected.

So, the idea is that we can collect data quickly, without interpolation, as
necessary/useful, and the list of attributes can be changed/revised as needed
after the fact. Nothing prevents the original agent from extracting attributes
from that data, but it is not required either.  Similarly, external agents can
add attributes by consulting with other sites/locations.

Why do it this way?  It enables rapid collection of data, in a decentralized
fashion.  Anyone can write a data collector and put data into the activity log.
Having additional information (such as source machine) may be useful to
disambiguate information further, but I am not certain unique identifiers are
really available at the present time.
That data can then be used by _transducers_ to augment the list of attributes
(or maybe "features" is a better word for it.)  For example:

* An image classifier that looks for cats in pictures could add an "attribute"
  to the list that indicates the picture contains an image of a cat.
* A hash generator can look at data objects and compute checksums/hashes over a
  file, allowing us to detect changes to a given file over time, for example.

So, I've been able to get things to generate in the format that I want:


## Meeting with Ada

I had my periodic meeting with Ada.  We discussed the project.  She suggested
messaging aps.

* Here's data I can get from various types of apps.  Find area of commonality.
* What is the granularity of the updates?
* Being able to show graphs of sizes, make it concrete, quantify, similarity of
  the data.  That will be a "very good contribution."  Perhaps VLDB.

